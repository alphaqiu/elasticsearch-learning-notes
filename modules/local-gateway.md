# 本地网关

本地网关模块存储集群状态并在集群重启后在整个集群中分配数据片

当你集群重启时，几个配置项影响你的分片恢复的表现。

首先，我们必须明白，如果什么也没配置将会发生什么。

想象一下假设你有10个节点，每个节点保存一个分片，主分片或者副分片，也就是有一个有5个主分片/1个副本 的索引。你需要停止整个集群进行休整（举个例子，为了安装一个新的驱动程序）。当你重启你的集群，很自然会出现5个节点已经起动了，还有5个还没启动的场景。

假设其它五个节点出问题，或者他们根本没有收到立即重启的命令。无论什么原因，你只要五个节点在线上。这五个节点会相互通信，选出一个master，从而形成一个集群，他们注意到数据不再均匀分布，因为有个节点在集群中丢失了，他们之间会立马启动分片复制。

最后，你的其它5个节点打开加入了集群，这些节点会发现他们的数据已经成为其它节点的副本了，所以他们删除本地数据（因为这份数据要么是多余的，要么是过时的）。然后整个集群重新进行平衡，因为集群的大小已经从5变成了10。

在整个过程中，你的节点会消耗磁盘和网盘，来回移动数据，因为没有更好的理由。对于有上T数据的大集群，这种数据的传输需要很长时间。如果等待所有的节点重启好了，整个集群再上线，所有的本地的数据都不需要移动。

> 以下静态设置项必须在每一个主节点上设置好，来控制当节点重启后尝试恢复集群状态和数据分片之前等待多长时间来选举主节点：

现在我们要告诉Ealsticsearch集群中应该有多少个节点，并且我们希望集群需要多久等待所有节点：

```yaml
gateway.expected_nodes: 10
gateway.recover_after_time: 5m
gateway.recover_after_nodes: 8
```

这意味着Elasticsearch会采取如下操作：

- 至少等待8个节点上线


- 等待5分钟，或者10个节点上线后，才进行数据恢复，这取决于哪个条件先达到。



- `gateway.expected_nodes`

  期望有多少个节点加入集群。一旦达到该数目，将立即恢复本地分片。默认值  `0`

- `gateway.expected_master_nodes`

期望有多少个主节点加入到集群。一旦达到该数目，将立即恢复本地分片。默认值 `0`

- `gateway.expected_data_nodes`

期望有多少个数据节点加入到集群。一旦达到该数目，将立即恢复本地分片。 默认值 `0`

- `gateway.recover_after_time`

如果设置了其中一个`expected_nodes`设置，并且集群重启后没有达到期望数量的节点，恢复进程将等待配置中指定时长后尝试恢复本地分片。默认 `5m`

一旦 `recover_after_time` 超时，恢复程序将依照以下条件处理：

- `gateway.recover_after_nodes`

  我们可以修改一个小配置就可以缓解这个事情，首先我们要给 Elasticsearch一个严格的限制:

  ```yaml
  gateway.recover_after_nodes: 8
  ```

  这将放置Elasticsearch进行数据恢复，在发现8个节点（数据节点或者master节点）之前。这个值的设定取决于个人喜好: 整个集群提供服务之前你希望有多少个节点在线？我们设置为8， 这意味着至少要有8个节点，该集群才可用。

- `gateway.recover_after_master_nodes`

  原理同上

- `gateway.recover_after_data_nodes`

  原理同上

> 这三个设置可以在集群重启的时候避免过多的分片交换。
>
> 这可能会让数据恢复从数个小时缩短为几秒钟。
>
> 这些配置只能设置在config/elasticsearch.yml文件中。
>
> 要生效这些设置，必须重启集群。